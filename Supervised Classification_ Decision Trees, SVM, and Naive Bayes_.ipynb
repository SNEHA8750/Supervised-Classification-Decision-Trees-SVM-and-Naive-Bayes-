{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzb8oIMKp47DoQzsckk9gq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","### **Question 1: What is Information Gain, and how is it used in Decision Trees?**\n","\n","**Answer:**\n","\n","**Information Gain (IG)** is a metric used in **Decision Trees** to decide which feature to split on at each step of the algorithm. It measures how much “information” or “purity” about the target variable is gained when the data is split based on a particular feature.\n","\n","In simple terms, it tells us **how well a feature separates the data into different classes**.\n","\n","\n","### **How It Works:**\n","\n","Decision Trees try to reduce **uncertainty** (or impurity) in the data. This uncertainty is measured using **Entropy**.\n","\n","* **Entropy (E)** measures randomness or impurity in the dataset.\n","\n","  * Formula:\n","    [\n","    Entropy(S) = - \\sum p_i \\log_2(p_i)\n","    ]\n","    where ( p_i ) = proportion of class *i* in the dataset *S*.\n","\n","* **Information Gain (IG)** tells us how much the entropy decreases after splitting the dataset based on a feature.\n","\n","  * Formula:\n","    [\n","    IG(S, A) = Entropy(S) - \\sum \\frac{|S_v|}{|S|} Entropy(S_v)\n","    ]\n","    where ( S_v ) are subsets of *S* created by splitting on attribute *A*.\n","\n","\n","### **Example:**\n","\n","Suppose we have a dataset of students:\n","\n","* Target: *Pass* or *Fail*\n","* Feature: *Study Hours*\n","\n","If splitting based on *Study Hours* results in groups where most students either pass or fail clearly (less mixed), the **Information Gain** will be high.\n","That means *Study Hours* is a good feature for splitting.\n","\n","\n","\n","### **Use in Decision Trees:**\n","\n","1. At each node, the Decision Tree calculates the **Information Gain** for every available feature.\n","2. The feature with the **highest Information Gain** is chosen for splitting.\n","3. This process continues recursively until:\n","\n","   * All data is classified, or\n","   * No significant gain can be achieved.\n","\n","\n","### **In Summary:**\n","\n","* **Information Gain** helps the Decision Tree select the most informative features.\n","* It ensures that each split results in **more pure** subsets.\n","* The higher the **Information Gain**, the better the feature is at classifying data.\n","\n","\n"],"metadata":{"id":"Y1Am3t15PKZy"}},{"cell_type":"markdown","source":["\n","\n","### **Question 2: What is the difference between Gini Impurity and Entropy?**\n","\n","**Answer:**\n","\n","Both **Gini Impurity** and **Entropy** are measures of impurity or disorder used by **Decision Tree algorithms** to decide the best feature for splitting the data. They both indicate how mixed the classes are in a given dataset, but they calculate impurity in slightly different ways.\n","\n","\n","\n","### **1. Definitions and Formulas**\n","\n","| Measure           | Formula                              | Meaning                                                                                                                                                     |\n","| ----------------- | ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| **Gini Impurity** | ( Gini = 1 - \\sum p_i^2 )            | Probability that a randomly chosen element would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the dataset. |\n","| **Entropy**       | ( Entropy = - \\sum p_i \\log_2(p_i) ) | Measures the average amount of information (or surprise) needed to identify the class of a randomly chosen element.                                         |\n","\n","Here, ( p_i ) = proportion of samples belonging to class *i*.\n","\n","\n","\n","### **2. Conceptual Difference**\n","\n","| Aspect          | **Gini Impurity**                                                    | **Entropy**                                                    |\n","| --------------- | -------------------------------------------------------------------- | -------------------------------------------------------------- |\n","| **Focus**       | Measures how often a randomly chosen element is misclassified.       | Measures the amount of information or uncertainty in the data. |\n","| **Computation** | Simpler and faster to compute.                                       | Slightly more complex due to logarithm calculation.            |\n","| **Range**       | 0 (pure) → 0.5 (max impurity for 2 classes).                         | 0 (pure) → 1 (max impurity for 2 classes).                     |\n","| **Behavior**    | Prefers larger splits and tends to isolate the most frequent class.  | Produces more balanced splits between classes.                 |\n","| **Used in**     | Default criterion in **CART (Classification and Regression Trees)**. | Used in **ID3** and **C4.5** Decision Tree algorithms.         |\n","\n","\n","\n","### **3. Strengths and Weaknesses**\n","\n","* **Gini Impurity:**\n","\n","  * ✅ Faster to compute (no log calculation).\n","  * ✅ Often yields similar results to entropy but with less computation.\n","  * ❌ Can slightly favor dominant classes.\n","\n","* **Entropy:**\n","\n","  * ✅ Has a strong theoretical basis in information theory.\n","  * ✅ Can produce more balanced splits.\n","  * ❌ Slightly slower due to logarithmic computation.\n","\n","\n","### **4. When to Use Which**\n","\n","* **Use Gini Impurity** when speed and simplicity are important (most practical cases).\n","* **Use Entropy** when you want a more information-theoretic approach or are using algorithms like ID3 or C4.5.\n","\n","\n","\n","### **In Summary**\n","\n","Both Gini Impurity and Entropy measure how pure a dataset is, and both lead to similar decision trees in practice.\n","The main difference lies in **how they measure impurity** and **their computational complexity** — Gini is faster, while Entropy is more theoretically grounded.\n","\n","\n"],"metadata":{"id":"qV0u8QnRPb2V"}},{"cell_type":"markdown","source":["\n","\n","### **Question 3: What is Pre-Pruning in Decision Trees?**\n","\n","**Answer:**\n","\n","**Pre-pruning** (also called **early stopping**) is a technique used in Decision Trees to **stop the tree from growing too deep** while it is being built.\n","The goal is to prevent **overfitting** — when a tree becomes too complex and starts memorizing the training data instead of learning general patterns.\n","\n","\n","\n","### **How It Works:**\n","\n","In pre-pruning, the tree-building process is **stopped early** based on certain conditions, **before** the model perfectly fits the training data.\n","Instead of allowing the tree to grow until every leaf is pure, we apply limits or thresholds such as:\n","\n","* **Maximum Depth (`max_depth`)** – Limits how deep the tree can go.\n","* **Minimum Samples per Split (`min_samples_split`)** – Minimum number of samples required to make a new split.\n","* **Minimum Samples per Leaf (`min_samples_leaf`)** – Minimum number of samples that must be at a leaf node.\n","* **Minimum Information Gain** – Stop splitting if the information gain is too small.\n","* **Maximum Number of Nodes** – Restricts how many total nodes the tree can have.\n","\n","When any of these criteria are met, the splitting stops even if the node is not perfectly pure.\n","\n","\n","\n","### **Example:**\n","\n","Suppose we set `max_depth = 3`.\n","Even if deeper splits could slightly improve accuracy on the training data, the algorithm stops at level 3 to prevent overfitting and improve generalization.\n","\n","\n","\n","### **Advantages of Pre-Pruning:**\n","\n","* ✅ Prevents overfitting early.\n","* ✅ Reduces training time and complexity.\n","* ✅ Makes the model simpler and easier to interpret.\n","\n","\n","\n","### **Disadvantages:**\n","\n","* ❌ Might stop too early and **underfit** the data.\n","* ❌ Choosing the right stopping criteria can be tricky.\n","\n","\n","\n","### **In Summary:**\n","\n","Pre-pruning stops the Decision Tree from growing too complex by applying early stopping rules during training.\n","It helps balance **model complexity** and **accuracy**, leading to better performance on unseen data.\n","\n","\n"],"metadata":{"id":"OGZMztLEPmRy"}},{"cell_type":"code","source":["### Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n","# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","import pandas as pd\n","\n","# 1. Load the dataset\n","data = load_iris()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# 2. Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# 3. Create and train the Decision Tree Classifier using Gini Impurity\n","clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# 4. Print model accuracy\n","accuracy = clf.score(X_test, y_test)\n","print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n","\n","# 5. Print feature importances\n","feature_importances = pd.Series(clf.feature_importances_, index=X.columns)\n","print(\"\\nFeature Importances:\")\n","print(feature_importances.sort_values(ascending=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruHBwhJnQBPn","executionInfo":{"status":"ok","timestamp":1761981019792,"user_tz":-330,"elapsed":3396,"user":{"displayName":"Sneha Pal","userId":"00700588384488618037"}},"outputId":"5c4f4356-a981-4d01-ef5f-6e6f843b7fd0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 100.0 %\n","\n","Feature Importances:\n","petal length (cm)    0.893264\n","petal width (cm)     0.087626\n","sepal width (cm)     0.019110\n","sepal length (cm)    0.000000\n","dtype: float64\n"]}]},{"cell_type":"markdown","source":["\n","\n","### **Question 5: What is a Support Vector Machine (SVM)?**\n","\n","**Answer:**\n","\n","A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification** and **regression** tasks.\n","Its main goal is to find the **best boundary (called a hyperplane)** that separates data points of different classes with the **maximum margin**.\n","\n","\n","\n","### **1. Key Idea:**\n","\n","SVM tries to find a line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions) that divides the data into distinct classes **as clearly as possible**.\n","\n","* The **margin** is the distance between the hyperplane and the nearest data points from each class.\n","* The data points that are **closest to the hyperplane** are called **Support Vectors**.\n","  These points play a crucial role in defining the position and orientation of the boundary.\n","\n","\n","\n","### **2. How It Works:**\n","\n","* SVM finds the **optimal hyperplane** that **maximizes the margin** between different classes.\n","* It uses mathematical optimization to ensure the separation is as wide as possible, reducing the chance of misclassification.\n","* For non-linear data, SVM uses **kernel functions** to map data into a higher-dimensional space where it becomes linearly separable.\n","\n","\n","\n","### **3. Common Kernel Functions:**\n","\n","| Kernel                          | Description                                      |\n","| ------------------------------- | ------------------------------------------------ |\n","| **Linear**                      | Works well when data is linearly separable.      |\n","| **Polynomial**                  | Suitable for curved decision boundaries.         |\n","| **RBF (Radial Basis Function)** | Handles complex, non-linear relationships.       |\n","| **Sigmoid**                     | Similar to a neural network activation function. |\n","\n","\n","### **4. Advantages:**\n","\n","* ✅ Works well on both linear and non-linear data.\n","* ✅ Effective in high-dimensional spaces.\n","* ✅ Robust against overfitting (especially with proper kernel choice).\n","\n","\n","\n","### **5. Disadvantages:**\n","\n","* ❌ Training can be slow on large datasets.\n","* ❌ Choosing the right kernel and parameters requires tuning.\n","* ❌ Less interpretable compared to simple models like Decision Trees.\n","\n","\n","\n","### **In Summary:**\n","\n","A **Support Vector Machine** is a powerful algorithm that finds the **optimal separating boundary** between different classes by maximizing the margin.\n","It is widely used for tasks such as **image classification, text categorization, and bioinformatics** due to its high accuracy and flexibility.\n","\n","\n"],"metadata":{"id":"UICg4e8eQg2d"}},{"cell_type":"markdown","source":["\n","\n","### **Question 6: What is the Kernel Trick in SVM?**\n","\n","**Answer:**\n","\n","The **Kernel Trick** is a mathematical technique used in **Support Vector Machines (SVMs)** to handle **non-linear data** efficiently.\n","It allows SVMs to separate data that **cannot be divided by a straight line** in the original feature space by mapping it to a **higher-dimensional space** — without actually performing complex computations in that space.\n","\n","\n","### **1. The Basic Idea:**\n","\n","In some datasets, data points from different classes are **not linearly separable**.\n","Instead of manually adding more features to make the data separable, the **Kernel Trick** helps by **implicitly transforming** the data into a higher dimension where a **linear boundary** can separate the classes.\n","\n","For example:\n","\n","* In 2D space, you cannot separate circular patterns with a straight line.\n","* The Kernel Trick maps this data to a 3D space where a plane can separate the classes easily.\n","\n","\n","\n","### **2. How It Works:**\n","\n","The Kernel Trick replaces the **dot product** of two feature vectors with a **kernel function** that computes this relationship in the higher-dimensional space.\n","\n","Mathematically:\n","[\n","K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n","]\n","where:\n","\n","* ( x_i, x_j ) = input feature vectors\n","* ( \\phi(x) ) = transformation to higher-dimensional space\n","* ( K ) = kernel function\n","\n","This allows the SVM to operate **as if** the data were transformed — without explicitly calculating the transformation.\n","\n","\n","\n","### **3. Common Kernel Functions:**\n","\n","| Kernel Type                                       | Formula                                   | Use Case                                             |       |   |       |                                     |\n","| ------------------------------------------------- | ----------------------------------------- | ---------------------------------------------------- | ----- | - | ----- | ----------------------------------- |\n","| **Linear Kernel**                                 | ( K(x, y) = x \\cdot y )                   | When data is linearly separable.                     |       |   |       |                                     |\n","| **Polynomial Kernel**                             | ( K(x, y) = (x \\cdot y + c)^d )           | When the relationship between classes is polynomial. |       |   |       |                                     |\n","| **RBF (Radial Basis Function) / Gaussian Kernel** | ( K(x, y) = e^{-\\gamma                    |                                                      | x - y |   | ^2} ) | For complex, non-linear boundaries. |\n","| **Sigmoid Kernel**                                | ( K(x, y) = \\tanh(\\alpha x \\cdot y + c) ) | Similar to neural network activation.                |       |   |       |                                     |\n","\n","\n","\n","### **4. Advantages of the Kernel Trick:**\n","\n","* ✅ Handles non-linear data efficiently.\n","* ✅ Avoids explicitly computing high-dimensional transformations (saves time and resources).\n","* ✅ Makes SVMs flexible and powerful for complex datasets.\n","\n","\n","\n","### **5. In Summary:**\n","\n","The **Kernel Trick** enables SVMs to solve non-linear problems by implicitly mapping data into a higher-dimensional space.\n","It allows the algorithm to create complex decision boundaries **without the heavy computational cost** of transforming the data directly.\n","\n"],"metadata":{"id":"wZL9ChAZQrcC"}},{"cell_type":"code","source":["### Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n","# Import necessary libraries\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Wine dataset\n","wine = datasets.load_wine()\n","X = wine.data\n","y = wine.target\n","\n","# 2. Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# 3. Create two SVM classifiers: one with Linear kernel and one with RBF kernel\n","svm_linear = SVC(kernel='linear', random_state=42)\n","svm_rbf = SVC(kernel='rbf', random_state=42)\n","\n","# 4. Train both classifiers\n","svm_linear.fit(X_train, y_train)\n","svm_rbf.fit(X_train, y_train)\n","\n","# 5. Make predictions on the test data\n","y_pred_linear = svm_linear.predict(X_test)\n","y_pred_rbf = svm_rbf.predict(X_test)\n","\n","# 6. Calculate accuracies\n","accuracy_linear = accuracy_score(y_test, y_pred_linear)\n","accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n","\n","# 7. Print comparison results\n","print(\"SVM Accuracy with Linear Kernel:\", round(accuracy_linear * 100, 2), \"%\")\n","print(\"SVM Accuracy with RBF Kernel:\", round(accuracy_rbf * 100, 2), \"%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIsfqpWJRVWU","executionInfo":{"status":"ok","timestamp":1761981335845,"user_tz":-330,"elapsed":864,"user":{"displayName":"Sneha Pal","userId":"00700588384488618037"}},"outputId":"51d5b4e4-da63-4750-98bd-5230ebcddf5b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy with Linear Kernel: 98.15 %\n","SVM Accuracy with RBF Kernel: 75.93 %\n"]}]},{"cell_type":"markdown","source":["\n","\n","### **Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n","\n","**Answer:**\n","\n","The **Naïve Bayes classifier** is a **supervised machine learning algorithm** based on **Bayes’ Theorem**.\n","It is mainly used for **classification tasks** such as text classification, spam detection, and sentiment analysis.\n","\n","\n","\n","### **1. What It Does:**\n","\n","Naïve Bayes predicts the class of a given sample by calculating the **probability** of each class and selecting the one with the **highest probability**.\n","It assumes that all features are **independent** of each other when given the class label — this is what makes it “naïve.”\n","\n","\n","\n","### **2. Bayes’ Theorem:**\n","\n","The algorithm uses **Bayes’ Theorem** to estimate probabilities:\n","\n","[\n","P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n","]\n","\n","Where:\n","\n","* ( P(A|B) ) = Probability of class *A* given data *B* (posterior probability)\n","* ( P(B|A) ) = Probability of data *B* given class *A* (likelihood)\n","* ( P(A) ) = Prior probability of class *A*\n","* ( P(B) ) = Probability of the data (evidence)\n","\n","\n","\n","### **3. Why It Is Called “Naïve”:**\n","\n","It is called **“Naïve”** because it **assumes all features are independent** of each other — meaning that the presence (or absence) of one feature does not affect another.\n","\n","In real-world data, this assumption is often **not true**, but the algorithm still performs surprisingly well in many situations.\n","\n","\n","### **4. Types of Naïve Bayes Classifiers:**\n","\n","| Type                        | Description                                                               |\n","| --------------------------- | ------------------------------------------------------------------------- |\n","| **Gaussian Naïve Bayes**    | Used when features are continuous and follow a normal distribution.       |\n","| **Multinomial Naïve Bayes** | Used for discrete counts like word frequencies in text data.              |\n","| **Bernoulli Naïve Bayes**   | Used for binary features (0 or 1), such as presence or absence of a word. |\n","\n","\n","### **5. Advantages:**\n","\n","* ✅ Simple and fast to train.\n","* ✅ Works well with high-dimensional data (like text).\n","* ✅ Performs well even with limited training data.\n","\n","\n","\n","### **6. Limitations:**\n","\n","* ❌ The independence assumption is unrealistic in many datasets.\n","* ❌ It may not perform well when features are highly correlated.\n","\n","\n","\n","### **In Summary:**\n","\n","The **Naïve Bayes classifier** uses Bayes’ Theorem to classify data based on probabilities.\n","It is called “Naïve” because it **simplifies computation** by assuming all features are **independent**, even though this assumption rarely holds true in real life.\n","\n","\n"],"metadata":{"id":"S9ky30bMRhEW"}},{"cell_type":"markdown","source":["\n","\n","### **Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n","\n","**Answer:**\n","\n","The **Naïve Bayes algorithm** has different versions depending on the **type of data** it is used for.\n","The three most common types are **Gaussian**, **Multinomial**, and **Bernoulli Naïve Bayes**.\n","Each version makes different assumptions about how the features are distributed.\n","\n","\n","\n","### **1. Gaussian Naïve Bayes**\n","\n","**Used for:** Continuous (numerical) data\n","**Assumption:** The features follow a **normal (Gaussian) distribution**\n","\n","* This means the data values are assumed to be spread around the mean in a bell-shaped curve.\n","* It calculates the probability of each class using the **mean and variance** of the data.\n","\n","**Example Use Case:**\n","Predicting whether a person has diabetes based on numerical features like age, blood pressure, and glucose level.\n","\n","**Formula Example:**\n","[\n","P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} e^{-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}}\n","]\n","\n","\n","\n","### **2. Multinomial Naïve Bayes**\n","\n","**Used for:** Discrete count data (especially text data)\n","**Assumption:** Features represent **frequencies or counts** — such as word counts in a document.\n","\n","* It works best when features are **non-negative integers**, such as the number of times a word appears.\n","* It is widely used in **text classification** tasks like **spam filtering** or **document categorization**.\n","\n","**Example Use Case:**\n","Classifying emails as spam or not spam based on word counts.\n","\n","\n","\n","### **3. Bernoulli Naïve Bayes**\n","\n","**Used for:** Binary/boolean features (0 or 1)\n","**Assumption:** Features indicate the **presence (1)** or **absence (0)** of a characteristic.\n","\n","* Instead of word counts, it checks whether a word **appears or not** in a document.\n","* Works well for **binary data** such as yes/no, true/false, or present/absent values.\n","\n","**Example Use Case:**\n","Text classification where each feature indicates whether a specific word occurs in an email (1 if present, 0 if not).\n","\n","\n","\n","### **4. Summary Table:**\n","\n","| **Type**                    | **Data Type**        | **Feature Example** | **Common Use Case**                            | **Distribution Assumed** |\n","| --------------------------- | -------------------- | ------------------- | ---------------------------------------------- | ------------------------ |\n","| **Gaussian Naïve Bayes**    | Continuous (numeric) | Height, Weight, Age | Medical or sensor data                         | Normal (Gaussian)        |\n","| **Multinomial Naïve Bayes** | Discrete (counts)    | Word counts in text | Text classification, spam detection            | Multinomial              |\n","| **Bernoulli Naïve Bayes**   | Binary (0/1)         | Word present/absent | Sentiment analysis, binary text classification | Bernoulli                |\n","\n","\n","\n","### **In Summary:**\n","\n","* **Gaussian NB** → For continuous data (uses mean and variance).\n","* **Multinomial NB** → For count-based data (e.g., word frequencies).\n","* **Bernoulli NB** → For binary data (e.g., presence or absence).\n","\n","Each version is tailored to handle a specific type of input, making **Naïve Bayes** a versatile algorithm for many kinds of classification problems.\n","\n"],"metadata":{"id":"BJm-_RScRy3y"}},{"cell_type":"code","source":["### Question 10: Breast Cancer Dataset : Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. Hint: Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n","# Import required libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Step 1: Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data        # Features\n","y = data.target      # Target labels (0 = malignant, 1 = benign)\n","\n","# Step 2: Split the dataset into 70% training and 30% testing data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Step 3: Create a Gaussian Naive Bayes classifier\n","gnb = GaussianNB()\n","\n","# Step 4: Train (fit) the model on the training data\n","gnb.fit(X_train, y_train)\n","\n","# Step 5: Make predictions on the test data\n","y_pred = gnb.predict(X_test)\n","\n","# Step 6: Evaluate the model's performance\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Step 7: Print results\n","print(\"=== Gaussian Naïve Bayes Classifier Results ===\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=data.target_names))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_Xy6yZYSJVa","executionInfo":{"status":"ok","timestamp":1761981577927,"user_tz":-330,"elapsed":37,"user":{"displayName":"Sneha Pal","userId":"00700588384488618037"}},"outputId":"4e3bbf3c-4871-4ac1-c5a7-7801b0560d75"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Gaussian Naïve Bayes Classifier Results ===\n","Accuracy: 0.9415\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","   malignant       0.93      0.90      0.92        63\n","      benign       0.95      0.96      0.95       108\n","\n","    accuracy                           0.94       171\n","   macro avg       0.94      0.93      0.94       171\n","weighted avg       0.94      0.94      0.94       171\n","\n"]}]}]}